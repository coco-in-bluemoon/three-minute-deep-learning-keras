{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import layers, models, optimizers\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def mse_4d(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=(1, 2, 3))\n",
    "\n",
    "def mse_4d_tf(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true), axis=(1, 2, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class GAN(models.Sequential):\n",
    "    def __init__(self, input_dim=64):\n",
    "\n",
    "        super(GAN, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.generator = self.GENERATOR()\n",
    "        self.discriminator = self.DSICRIMINATOR()\n",
    "\n",
    "        self.add(self.generator)\n",
    "        self.discriminator.trainable = False\n",
    "        self.add(self.discriminator)\n",
    "\n",
    "        self.compile_all()\n",
    "\n",
    "    def compile_all(self):\n",
    "        d_optimizer = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "        g_optimizer = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "\n",
    "        self.generator.compile(loss=mse_4d, optimizer='SGD')\n",
    "        self.compile(loss='binary_crossentropy', optimizer=g_optimizer)\n",
    "        self.discriminator.trainable = True\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=d_optimizer)\n",
    "\n",
    "    def GENERATOR(self):\n",
    "        input_dim = self.input_dim\n",
    "\n",
    "        model = models.Sequential()\n",
    "        model.add(\n",
    "            layers.Dense(1024, activation='tanh', input_dim=input_dim)\n",
    "        )\n",
    "        model.add(\n",
    "            layers.Dense(7*7*128, activation='tanh')\n",
    "        )\n",
    "        model.add(\n",
    "            layers.BatchNormalization()\n",
    "        )\n",
    "        model.add(\n",
    "            layers.Reshape((7, 7, 128), input_shape=(7*7*128,))\n",
    "        )\n",
    "        model.add(\n",
    "            layers.UpSampling2D(size=(2, 2))\n",
    "        )\n",
    "        model.add(\n",
    "            layers.Conv2D(64, (5, 5), padding='same', activation='tanh')\n",
    "        )\n",
    "        model.add(\n",
    "            layers.UpSampling2D(size=(2, 2))\n",
    "        )\n",
    "        model.add(\n",
    "            layers.Conv2D(1, (5, 5), padding='same', activation='tanh')\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def DISCRIMINATOR(self):\n",
    "        model = models.Sequential()\n",
    "\n",
    "        model.add(\n",
    "            layers.Conv2D(64, (5, 5), padding='same', activation='tanh')\n",
    "        )\n",
    "        model.add(\n",
    "            layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        )\n",
    "        model.add(\n",
    "            layers.Conv2D(128, (5, 5), activation='tanh')\n",
    "        )\n",
    "        model.add(\n",
    "            layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        )\n",
    "        model.add(\n",
    "            layers.Flatten()\n",
    "        )\n",
    "        model.add(\n",
    "            layers.Dense(1024, activation='tanh')\n",
    "        )\n",
    "        model.add(\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_z(self, ln):\n",
    "        input_dim = self.input_dim\n",
    "        return np.random.uniform(-1, 1, (ln, input_dim))\n",
    "\n",
    "    def train_both(self, x):\n",
    "        ln = x.shape[0]\n",
    "\n",
    "        z = self.get_z(ln)\n",
    "        w = self.generator.predict(z, verbose=0)\n",
    "        xw = np.concatenate((x, w))\n",
    "        y2 = [1] * ln + [0] * ln\n",
    "        d_loss = self.discriminator.train_on_batch(xw, y2)\n",
    "\n",
    "        z = self.get_z(ln)\n",
    "        self.discriminator.trainable = False\n",
    "        g_loss = self.train_on_batch(z, [1] * ln)\n",
    "        self.discriminator.trainable = True\n",
    "\n",
    "        return d_loss, g_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import math\n",
    "import os\n",
    "from keras.datasets import mnist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num) / width))\n",
    "    shape = generated_images.shape[2:]\n",
    "\n",
    "    image = np.zeros((height * shape[0], width * shape[1]), dtype=generated_images.dtype)\n",
    "\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index / width)\n",
    "        j = index % width\n",
    "\n",
    "        image[i*shape[0]:(i+1)*shape[0],j*shape[1]:(j+1)*shape[1]] = img[0, :, :]\n",
    "    return image\n",
    "\n",
    "def get_x(X_train, index, BATCH_SIZE):\n",
    "    return X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "\n",
    "def save_images(generated_images, output_fold, epoch, index):\n",
    "    image = combine_images(generated_images)\n",
    "    image = image * 127.5 + 127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(f'{output_fold}/{str(epoch)}_{str(index)}.png')\n",
    "\n",
    "def load_data(n_train):\n",
    "    (X_train, y_train), (_, _) = mnist.load_data()\n",
    "    return X_train[:n_train]\n",
    "\n",
    "def train(args):\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    output_fold = args.output_fold\n",
    "    input_dim = args.input_dim\n",
    "    n_train = args.n_train\n",
    "\n",
    "    os.makedirs(output_fold, exist_ok=True)\n",
    "    print('output_fold is', output_fold)\n",
    "\n",
    "    X_train = load_data(n_train)\n",
    "\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = X_train.reshape(X_train.shape + (1, ))\n",
    "\n",
    "    gan = GAN(input_dim)\n",
    "\n",
    "    d_loss_ll = []\n",
    "    g_loss_ll = []\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch is', epoch)\n",
    "        print('Number of batches', int(X_train.shape[0] / BATCH_SIZE))\n",
    "\n",
    "        d_loss_l = []\n",
    "        g_loss_l = []\n",
    "\n",
    "        for index in range(int(X_train.shape[0] / BATCH_SIZE)):\n",
    "            x = get_x(X_train, index, BATCH_SIZE)\n",
    "\n",
    "            d_loss, g_loss = gan.train_both(x)\n",
    "\n",
    "            d_loss_l.append(d_loss_l)\n",
    "            g_loss_l.append(g_loss_l)\n",
    "\n",
    "            if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "                z = gan.get_z(x.shape[0])\n",
    "                w = gan.generator.predict(z, verbose=0)\n",
    "                save_images(w, output_fold, epoch, 0)\n",
    "\n",
    "            d_loss_ll.append(d_loss_l)\n",
    "            g_loss_ll.append(g_loss_l)\n",
    "\n",
    "        gan.generator.save_weights(f'{output_fold}/generator', True)\n",
    "        gan.discriminator.save_weights(f'{output_fold}/discriminator', True)\n",
    "\n",
    "        np.savetxt(f'{output_fold}/d_loss', d_loss_ll)\n",
    "        np.savetxt(f'{output_fold}/g_loss', g_loss_ll)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import argparse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=16, help='Batch size for the networks')\n",
    "    parser.add_argument('--epochs', type=int, default=1000, help='Epochs for the networks')\n",
    "    parser.add_argument('--output_fold', type=str, default='GAN_OUT', help='Output fold to save the results')\n",
    "    parser.add_argument('--input_dim', type=int, default=10, help='Input dimension for the generator')\n",
    "    parser.add_argument('--n_train', type=int, default=32, help='The number of training data')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    train(args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE] [--epochs EPOCHS]\n",
      "                             [--output_fold OUTPUT_FOLD]\n",
      "                             [--input_dim INPUT_DIM] [--n_train N_TRAIN]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/dhsong/Library/Jupyter/runtime/kernel-45b1a329-8bcd-46f3-b1a0-573a023c36d1.json\n",
      "/Users/dhsong/Workspaces/Study/Books/three-minute-deep-learning-keras/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 2\n"
     ]
    }
   ],
   "source": [
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}